behaviors:

    PlayerX:
        trainer_type: poca
        hyperparameters:
          batch_size: 32 # A smaller batch better for short episodes, as it allows for more frequent updates
          buffer_size: 640
          learning_rate: 0.0003  # moderate learning rate
          beta: 0.02  # explore a lot
          epsilon: 0.2 # 0.2 the clip range determines the trust region size, effectively limiting the policy update step
          lambd: 0.99 # For short episodes, setting lambda to a value closer to 1, which can reduce the variance in the advantage estimates.
          num_epoch: 10 # high number of epochs as short episodes so keep iterating over the same collected set more frequently to make most of it
          learning_rate_schedule: constant
          beta_schedule: constant
          epsilon_schedule: constant
        network_settings:
          normalize: false
          hidden_units: 128
          num_layers: 2
          vis_encode_type: simple
          goal_conditioning_type: none # no goal
          deterministic: false
        reward_signals:
          extrinsic:
            gamma: 0.80 # (closer to 0) prioritizes immediate rewards. In short episodes, that's what we want
            strength: 1.0
        keep_checkpoints: 30
        checkpoint_interval: 10000
        max_steps: 450000 # over this looks like it starts over fitting
        time_horizon: 64
        summary_freq: 500
        threaded: false
        self_play:
            window: 5 # save past 5 snapshots to train against
            play_against_latest_model_ratio: 0.5 
            save_steps: 1000 # keep opponent variance not to extreme
            swap_steps: 2000 # ghost opponent changes
            team_change: 10000 # how often to swap opponent policy so pick one of the 5 times each training swap


    PlayerO:
        trainer_type: poca
        hyperparameters:
          batch_size: 32 # A smaller batch better for short episodes, as it allows for more frequent updates
          buffer_size: 640
          learning_rate: 0.0003  # moderate learning rate
          beta: 0.02 # explore a lot
          epsilon: 0.2 # 0.2 the clip range determines the trust region size, effectively limiting the policy update step
          lambd: 0.99 # For short episodes, setting lambda to a value closer to 1, which can reduce the variance in the advantage estimates.
          num_epoch: 10 # high number of epochs as short episodes so keep iterating over the same collected set more frequently to make most of it
          learning_rate_schedule: constant
          beta_schedule: constant
          epsilon_schedule: constant
        network_settings:
          normalize: false
          hidden_units: 128
          num_layers: 2
          vis_encode_type: simple
          goal_conditioning_type: none # no goal
          deterministic: false
        reward_signals:
          extrinsic:
            gamma: 0.80 # (closer to 0) prioritizes immediate rewards. In short episodes, that's what we want
            strength: 1.0
        keep_checkpoints: 30
        checkpoint_interval: 10000
        max_steps: 450000 # over this looks like it starts over fitting
        time_horizon: 64
        summary_freq: 500
        threaded: false
        self_play:
            window: 5 # save past 5 snapshots to train against
            play_against_latest_model_ratio: 0.5 
            save_steps: 1000 # keep opponent variance not to extreme
            swap_steps: 2000 # ghost opponent changes
            team_change: 10000 # how often to swap opponent policy so pick one of the 5 times each training swap
      
